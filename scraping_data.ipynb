{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/open/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/open/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quarterfinals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/quarterfinals/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?quarterfinal=211&division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?quarterfinal=211&division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/quarterfinals/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?quarterfinal=211&division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?quarterfinal=211&division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semifinals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/semifinals/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "semifinals = [215, 217, 218, 219, 220, 221, 222]\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each semifinal\n",
    "for semifinal in semifinals:\n",
    "    # Iterate over each gender\n",
    "    for gender in genders:\n",
    "        # Create the initial API URL to fetch the data for the first page\n",
    "        url = f\"{base_url}?semifinal={semifinal}&division={gender}&page=1\"\n",
    "        try:\n",
    "            # Fetch the response for the first page\n",
    "            response = requests.get(url).json()\n",
    "            # Extract the total number of pages from the response\n",
    "            total_pages = response['pagination']['totalPages']\n",
    "            # Iterate over each page\n",
    "            for page in range(1, total_pages + 1):\n",
    "                # Create the API URL for each page\n",
    "                url = f\"{base_url}?semifinal={semifinal}&division={gender}&page={page}\"\n",
    "                try:\n",
    "                    # Fetch the response for the current page\n",
    "                    response = requests.get(url).json()\n",
    "                    # Extract the data for each row in the leaderboardRows\n",
    "                    for row in response['leaderboardRows']:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['entrant'].copy()\n",
    "                        row_data['overallRank'] = row['overallRank']\n",
    "                        row_data['overallScore'] = row['overallScore']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "                except Exception as e:\n",
    "                    # Handle any errors that occur during the API request for a specific page\n",
    "                    print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the API request for fetching total_pages\n",
    "            print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/semifinals/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "semifinals = [215, 217, 218, 219, 220, 221, 222]\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each semifinal\n",
    "for semifinal in semifinals:\n",
    "    # Iterate over each gender\n",
    "    for gender in genders:\n",
    "        # Create the initial API URL to fetch the data for the first page\n",
    "        url = f\"{base_url}?semifinal={semifinal}&division={gender}&page=1\"\n",
    "        try:\n",
    "            # Fetch the response for the first page\n",
    "            response = requests.get(url).json()\n",
    "            # Extract the total number of pages from the response\n",
    "            total_pages = response['pagination']['totalPages']\n",
    "            # Iterate over each page\n",
    "            for page in range(1, total_pages + 1):\n",
    "                # Create the API URL for each page\n",
    "                url = f\"{base_url}?semifinal={semifinal}&division={gender}&page={page}\"\n",
    "                try:\n",
    "                    # Fetch the response for the current page\n",
    "                    response = requests.get(url).json()\n",
    "                    # Extract the data for each row in the leaderboardRows\n",
    "                    for row in response['leaderboardRows']:\n",
    "                        # Iterate over each ordinal\n",
    "                        for ordinal in ordinals:\n",
    "                            # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                            row_data = row['scores'][ordinal].copy()\n",
    "                            row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                            # Append the row data to the data_list\n",
    "                            data_list.append(row_data)\n",
    "                except Exception as e:\n",
    "                    # Handle any errors that occur during the API request for a specific page\n",
    "                    print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the API request for fetching total_pages\n",
    "            print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Games"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/games/2023/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/open/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/open/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quarterfinals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/quarterfinalsindividual/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/quarterfinalsindividual/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semifinals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/semifinals/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "semifinals = [195, 196, 197, 198, 199, 202, 203, 204, 205, 206, 209]\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each semifinal\n",
    "for semifinal in semifinals:\n",
    "    # Iterate over each gender\n",
    "    for gender in genders:\n",
    "        # Create the initial API URL to fetch the data for the first page\n",
    "        url = f\"{base_url}?semifinal={semifinal}&division={gender}&page=1\"\n",
    "        try:\n",
    "            # Fetch the response for the first page\n",
    "            response = requests.get(url).json()\n",
    "            # Extract the total number of pages from the response\n",
    "            total_pages = response['pagination']['totalPages']\n",
    "            # Iterate over each page\n",
    "            for page in range(1, total_pages + 1):\n",
    "                # Create the API URL for each page\n",
    "                url = f\"{base_url}?semifinal={semifinal}&division={gender}&page={page}\"\n",
    "                try:\n",
    "                    # Fetch the response for the current page\n",
    "                    response = requests.get(url).json()\n",
    "                    # Extract the data for each row in the leaderboardRows\n",
    "                    for row in response['leaderboardRows']:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['entrant'].copy()\n",
    "                        row_data['overallRank'] = row['overallRank']\n",
    "                        row_data['overallScore'] = row['overallScore']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "                except Exception as e:\n",
    "                    # Handle any errors that occur during the API request for a specific page\n",
    "                    print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the API request for fetching total_pages\n",
    "            print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/semifinals/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "semifinals = [195, 196, 197, 198, 199, 202, 203, 204, 205, 206, 209]\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each semifinal\n",
    "for semifinal in semifinals:\n",
    "    # Iterate over each gender\n",
    "    for gender in genders:\n",
    "        # Create the initial API URL to fetch the data for the first page\n",
    "        url = f\"{base_url}?semifinal={semifinal}&division={gender}&page=1\"\n",
    "        try:\n",
    "            # Fetch the response for the first page\n",
    "            response = requests.get(url).json()\n",
    "            # Extract the total number of pages from the response\n",
    "            total_pages = response['pagination']['totalPages']\n",
    "            # Iterate over each page\n",
    "            for page in range(1, total_pages + 1):\n",
    "                # Create the API URL for each page\n",
    "                url = f\"{base_url}?semifinal={semifinal}&division={gender}&page={page}\"\n",
    "                try:\n",
    "                    # Fetch the response for the current page\n",
    "                    response = requests.get(url).json()\n",
    "                    # Extract the data for each row in the leaderboardRows\n",
    "                    for row in response['leaderboardRows']:\n",
    "                        # Iterate over each ordinal\n",
    "                        for ordinal in ordinals:\n",
    "                            # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                            row_data = row['scores'][ordinal].copy()\n",
    "                            row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                            # Append the row data to the data_list\n",
    "                            data_list.append(row_data)\n",
    "                except Exception as e:\n",
    "                    # Handle any errors that occur during the API request for a specific page\n",
    "                    print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the API request for fetching total_pages\n",
    "            print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Games"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/games/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/games/2022/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2021"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/open/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/open/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quarterfinals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/quarterfinalsindividual/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/quarterfinalsindividual/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semifinals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/semifinals/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "semifinals = [176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187]\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each semifinal\n",
    "for semifinal in semifinals:\n",
    "    # Iterate over each gender\n",
    "    for gender in genders:\n",
    "        # Create the initial API URL to fetch the data for the first page\n",
    "        url = f\"{base_url}?semifinal={semifinal}&division={gender}&page=1\"\n",
    "        try:\n",
    "            # Fetch the response for the first page\n",
    "            response = requests.get(url).json()\n",
    "            # Extract the total number of pages from the response\n",
    "            total_pages = response['pagination']['totalPages']\n",
    "            # Iterate over each page\n",
    "            for page in range(1, total_pages + 1):\n",
    "                # Create the API URL for each page\n",
    "                url = f\"{base_url}?semifinal={semifinal}&division={gender}&page={page}\"\n",
    "                try:\n",
    "                    # Fetch the response for the current page\n",
    "                    response = requests.get(url).json()\n",
    "                    # Extract the data for each row in the leaderboardRows\n",
    "                    for row in response['leaderboardRows']:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['entrant'].copy()\n",
    "                        row_data['overallRank'] = row['overallRank']\n",
    "                        row_data['overallScore'] = row['overallScore']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "                except Exception as e:\n",
    "                    # Handle any errors that occur during the API request for a specific page\n",
    "                    print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the API request for fetching total_pages\n",
    "            print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/semifinals/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "semifinals = [176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187]\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each semifinal\n",
    "for semifinal in semifinals:\n",
    "    # Iterate over each gender\n",
    "    for gender in genders:\n",
    "        # Create the initial API URL to fetch the data for the first page\n",
    "        url = f\"{base_url}?semifinal={semifinal}&division={gender}&page=1\"\n",
    "        try:\n",
    "            # Fetch the response for the first page\n",
    "            response = requests.get(url).json()\n",
    "            # Extract the total number of pages from the response\n",
    "            total_pages = response['pagination']['totalPages']\n",
    "            # Iterate over each page\n",
    "            for page in range(1, total_pages + 1):\n",
    "                # Create the API URL for each page\n",
    "                url = f\"{base_url}?semifinal={semifinal}&division={gender}&page={page}\"\n",
    "                try:\n",
    "                    # Fetch the response for the current page\n",
    "                    response = requests.get(url).json()\n",
    "                    # Extract the data for each row in the leaderboardRows\n",
    "                    for row in response['leaderboardRows']:\n",
    "                        # Iterate over each ordinal\n",
    "                        for ordinal in ordinals:\n",
    "                            # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                            row_data = row['scores'][ordinal].copy()\n",
    "                            row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                            # Append the row data to the data_list\n",
    "                            data_list.append(row_data)\n",
    "                except Exception as e:\n",
    "                    # Handle any errors that occur during the API request for a specific page\n",
    "                    print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the API request for fetching total_pages\n",
    "            print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Games"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/games/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                    row_data = row['entrant'].copy()\n",
    "                    row_data['overallRank'] = row['overallRank']\n",
    "                    row_data['overallScore'] = row['overallScore']\n",
    "                    # Append the row data to the data_list\n",
    "                    data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Athletes scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://c3po.crossfit.com/api/leaderboards/v2/competitions/games/2021/leaderboards\"\n",
    "\n",
    "data_list = []\n",
    "genders = [1, 2]\n",
    "ordinals = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Iterate over each gender\n",
    "for gender in genders:\n",
    "    # Create the initial API URL to fetch the data for the first page\n",
    "    url = f\"{base_url}?division={gender}&page=1\"\n",
    "    try:\n",
    "        # Fetch the response for the first page\n",
    "        response = requests.get(url).json()\n",
    "        # Extract the total number of pages from the response\n",
    "        total_pages = response['pagination']['totalPages']\n",
    "        # Iterate over each page\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Create the API URL for each page\n",
    "            url = f\"{base_url}?division={gender}&page={page}\"\n",
    "            try:\n",
    "                # Fetch the response for the current page\n",
    "                response = requests.get(url).json()\n",
    "                # Extract the data for each row in the leaderboardRows\n",
    "                for row in response['leaderboardRows']:\n",
    "                    # Iterate over each ordinal\n",
    "                    for ordinal in ordinals:\n",
    "                        # Copy the entrant data and add additional fields for overallRank and overallScore\n",
    "                        row_data = row['scores'][ordinal].copy()\n",
    "                        row_data['competitorId'] = row['entrant']['competitorId']\n",
    "                        # Append the row data to the data_list\n",
    "                        data_list.append(row_data)\n",
    "            except Exception as e:\n",
    "                # Handle any errors that occur during the API request for a specific page\n",
    "                print(f\"Error occurred while fetching data for semifinal={semifinal}, gender={gender}, page={page}: {e}\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the API request for fetching total_pages\n",
    "        print(f\"Error occurred while fetching total_pages for semifinal={semifinal}, gender={gender}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the collected data_list\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
